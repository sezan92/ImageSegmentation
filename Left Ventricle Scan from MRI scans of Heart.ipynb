{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MD Muhaimin Rahman\n",
    "contact: sezan92[at]gmail[dot]com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I am trying to make you go through Detecting Left Ventricles from Heart MRI scan images. The project is inspired by [[1](#ref1)]. The data preparation and function codeblocks are copied from there. The tensorflow implementation and visualization is added by me "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dicom, lmdb, cv2, re, sys\n",
    "import os, fnmatch, shutil, subprocess\n",
    "from IPython.utils import io\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore') # we ignore a RuntimeWarning produced from dividing by zero\n",
    "print(\"\\nSuccessfully imported packages, hooray!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation. Note, this repository doesn't have the dataset. To download the data , please go to [[1](#ref1)] . And change SUNNY_BROOK_ROOT_PATH to where the images are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Data Preparation\n",
    "SAX_SERIES = {\n",
    "    # challenge training\n",
    "    \"SC-HF-I-1\": \"0004\",\n",
    "    \"SC-HF-I-2\": \"0106\",\n",
    "    \"SC-HF-I-4\": \"0116\",\n",
    "    \"SC-HF-I-40\": \"0134\",\n",
    "    \"SC-HF-NI-3\": \"0379\",\n",
    "    \"SC-HF-NI-4\": \"0501\",\n",
    "    \"SC-HF-NI-34\": \"0446\",\n",
    "    \"SC-HF-NI-36\": \"0474\",\n",
    "    \"SC-HYP-1\": \"0550\",\n",
    "    \"SC-HYP-3\": \"0650\",\n",
    "    \"SC-HYP-38\": \"0734\",\n",
    "    \"SC-HYP-40\": \"0755\",\n",
    "    \"SC-N-2\": \"0898\",\n",
    "    \"SC-N-3\": \"0915\",\n",
    "    \"SC-N-40\": \"0944\",\n",
    "}\n",
    "\n",
    "SUNNYBROOK_ROOT_PATH = \"/home/sezan92/Documents/ImageSegmentation\"\n",
    "\n",
    "TRAIN_CONTOUR_PATH = os.path.join(SUNNYBROOK_ROOT_PATH,\n",
    "                            \"Sunnybrook Cardiac MR Database ContoursPart3\",\n",
    "                            \"TrainingDataContours\")\n",
    "TRAIN_IMG_PATH = os.path.join(SUNNYBROOK_ROOT_PATH,\n",
    "                        \"challenge_training\")\n",
    "TEST_IMG_PATH = os.path.join(SUNNYBROOK_ROOT_PATH,\n",
    "                             \"challange_validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_case(case):\n",
    "    toks = case.split(\"-\")\n",
    "    def shrink_if_number(x):\n",
    "        try:\n",
    "            cvt = int(x)\n",
    "            return str(cvt)\n",
    "        except ValueError:\n",
    "            return x\n",
    "    return \"-\".join([shrink_if_number(t) for t in toks])\n",
    "\n",
    "class Contour(object):\n",
    "    def __init__(self, ctr_path):\n",
    "        self.ctr_path = ctr_path\n",
    "        match = re.search(r\"/([^/]*)/contours-manual/IRCCI-expert/IM-0001-(\\d{4})-icontour-manual.txt\", ctr_path)\n",
    "        self.case = shrink_case(match.group(1))\n",
    "        self.img_no = int(match.group(2))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Contour for case %s, image %d>\" % (self.case, self.img_no)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "\n",
    "def load_contour(contour, img_path):\n",
    "    filename = \"IM-%s-%04d.dcm\" % (SAX_SERIES[contour.case], contour.img_no)\n",
    "    full_path = os.path.join(img_path, contour.case, filename)\n",
    "    f = dicom.read_file(full_path)\n",
    "    img = f.pixel_array.astype(np.int)\n",
    "    ctrs = np.loadtxt(contour.ctr_path, delimiter=\" \").astype(np.int)\n",
    "    label = np.zeros_like(img, dtype=\"uint8\")\n",
    "    cv2.fillPoly(label, [ctrs], 1)\n",
    "    return img, label\n",
    "\n",
    "    \n",
    "def get_all_contours(contour_path):\n",
    "    \n",
    "    contours = [os.path.join(dirpath, f)\n",
    "        for dirpath, dirnames, files in os.walk(contour_path)\n",
    "        for f in fnmatch.filter(files, 'IM-0001-*-icontour-manual.txt')]\n",
    "    print(\"Shuffle data\")\n",
    "    np.random.shuffle(contours)\n",
    "    print(\"Number of examples: {:d}\".format(len(contours)))\n",
    "    extracted = map(Contour, contours)\n",
    "    return extracted\n",
    "\n",
    "def export_all_contours(contours, img_path, lmdb_img_name, lmdb_label_name):        \n",
    "    for lmdb_name in [lmdb_img_name, lmdb_label_name]:\n",
    "        db_path = os.path.abspath(lmdb_name)\n",
    "        if os.path.exists(db_path):\n",
    "            shutil.rmtree(db_path)\n",
    "    batchsz = 100\n",
    "    print(\"Processing {:d} images and labels...\".format(len(contours)))\n",
    "\n",
    "    imgs, labels = [], []\n",
    "    for i in xrange(int(np.ceil(len(contours) / float(batchsz)))):\n",
    "        batch = contours[(batchsz*i):(batchsz*(i+1))]\n",
    "        if len(batch) == 0:\n",
    "            break\n",
    "        \n",
    "        for idx,ctr in enumerate(batch):\n",
    "            img, label = load_contour(ctr, img_path)\n",
    "            imgs.append(img.flatten())\n",
    "            labels.append(label.flatten())\n",
    "            imgsAll.append(img)\n",
    "            labelsAll.append(label)\n",
    "            #if idx % 20 == 0:\n",
    "                #print ctr\n",
    "                #plt.imshow(img)\n",
    "                #plt.show()\n",
    "                #plt.imshow(label)\n",
    "                #plt.show()\n",
    "    return imgs,labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here , the FCN (Fully Convolutional Network)[[2](#ref2)] Architecture is used. The tensorflow implementation is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_notImage = tf.placeholder(tf.float32,name =\"inputs_notImage\",shape = (None,65536))\n",
    "inputs_ = tf.reshape(inputs_notImage,[-1,256,256,1])\n",
    "targets_notImage = tf.placeholder(tf.float32,name = 'targets',shape = (None,65536))\n",
    "targets_ = tf.reshape(targets_notImage,[-1,256,256,1])\n",
    "# 256 x 256\n",
    "conv1= tf.layers.conv2d(inputs_,16,(5,5),2,padding= 'same',activation= tf.nn.relu)\n",
    "# 256 x 256 x16\n",
    "pool1=tf.layers.max_pooling2d(conv1,(2,2),2,padding='same' ) \n",
    "# 128 x 128 x16\n",
    "conv2 = tf.layers.conv2d(pool1,8,(5,5),2,padding='same',activation= tf.nn.relu)\n",
    "#128 x128x8\n",
    "pool2 = tf.layers.max_pooling2d(conv2,(2,2),2,padding='same')\n",
    "#64 x64 x8\n",
    "conv3 = tf.layers.conv2d(pool2,8,(3,3),1,padding='same',activation=tf.nn.relu)\n",
    "#64 x 64 x8\n",
    "conv4 = tf.layers.conv2d(conv3,8,(3,3),1,padding = 'same',activation= tf.nn.relu)\n",
    "# 64 x 64 x 8\n",
    "drop = tf.layers.dropout(inputs = conv4,rate= 0.1)\n",
    "# 64 x 64 x8\n",
    "score_classes = tf.layers.conv2d(drop,8,(1,1),1,padding='same')\n",
    "#64 x 64 x8\n",
    "upsample = tf.image.resize_nearest_neighbor(score_classes,(256,256))\n",
    "#256 x 256 x 16\n",
    "logits = tf.layers.conv2d(upsample,1,(3,3),padding='same')\n",
    "\n",
    "decoded = tf.nn.sigmoid(logits,name = 'decoded')\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "correct_pred = tf.equal(logits,targets_)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.1\n",
    "print(\"Mapping ground truth contours to images...\")\n",
    "ctrs = get_all_contours(TRAIN_CONTOUR_PATH)\n",
    "val_ctrs = ctrs[0:int(SPLIT_RATIO*len(ctrs))]\n",
    "train_ctrs = ctrs[int(SPLIT_RATIO*len(ctrs)):]\n",
    "print(\"Done mapping ground truth contours to images\")\n",
    "print(\"\\nBuilding LMDB for train...\")\n",
    "trainImgs,trainLabels=export_all_contours(train_ctrs, TRAIN_IMG_PATH, \"train_images_lmdb\", \"train_labels_lmdb\")\n",
    "print(\"\\nGot Train Data Set...\")\n",
    "valImgs,valLabels=export_all_contours(val_ctrs, TRAIN_IMG_PATH, \"val_images_lmdb\", \"val_labels_lmdb\")\n",
    "print(\"\\nGot Validation Data Set...\")\n",
    "print(\"Starting Tensorflow magic\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    count =0\n",
    "    batch_size =10\n",
    "    epochs = 100\n",
    "    for i in range(epochs):\n",
    "        print \"epoch \" +str(i)  \n",
    "        count =0\n",
    "        while count in range(0,len(trainImgs)):\n",
    "            feed = {inputs_notImage:trainImgs[count:count+batch_size],\n",
    "                    targets_notImage:trainLabels[count:count+batch_size]}\n",
    "            loss, _ = sess.run([cost, opt], feed_dict=feed)\n",
    "            print \"train Loss \"+ str(loss) + \"with epoch \"+str(i)\n",
    "            resultLabels=[]\n",
    "            if i == 99:\n",
    "                if count==230:\n",
    "                    for image in range(len(valImgs)):\n",
    "                        print \"Validation Image\"\n",
    "                        feed = {inputs_notImage:valImgs[image].reshape((1,65536)),\n",
    "                               }\n",
    "                        label= sess.run(decoded,feed_dict=feed)\n",
    "                        resultLabels.append(label.reshape((256,256)))\n",
    "                        #print loss\n",
    "                        #plt.figure\n",
    "                        #plt.imshow(label.reshape((256,256)))\n",
    "                        #plt.figure()\n",
    "                        #plt.imshow(valLabels[image].reshape(256,256))\n",
    "                        #plt.figure()\n",
    "            count = count+batch_size\n",
    "pickle.dump(resultLabels,open('Epoch100_labels.pkl','wb'))\n",
    "pickle.dump(valImgs,open('ValidationImages.pkl','wb'))\n",
    "pickle.dump(valLabels,open('ValidationLabels.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pickle.load(open('Epoch100_labels.pkl','r'))\n",
    "for i in range(len(valImgs)):\n",
    "    plt.figure()\n",
    "    plt.subplot(1,3,1)\n",
    "    a1 = valImgs[i].reshape((256,256))\n",
    "    a2 = a1\n",
    "    a3 = a1\n",
    "    plt.imshow(a1)\n",
    "    plt.title(\"Image\")\n",
    "    plt.subplot(1,3,2)\n",
    "    b =valLabels[i].reshape((256,256))\n",
    "    for row in range(b.shape[0]):\n",
    "        for col in range(b.shape[1]):\n",
    "            if b[row][col]==1:\n",
    "                a2[row][col]=500\n",
    "            plt.imshow(a2)\n",
    "            plt.title(\"Labeled\")\n",
    "            plt.subplot(1,3,3)\n",
    "            c = pred[i].reshape((256,256))\n",
    "    for row in range(c.shape[0]):\n",
    "        for col in range(c.shape[1]):\n",
    "            if c[row][col]>0.25:\n",
    "                a3[row][col]=500\n",
    "            plt.imshow(a3)\n",
    "            plt.title(\"Prediction\")              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three samples. The Lest most is the real MRI scan Images, the middle ones are the labels , with the labels indicating Left ventricles. The right most images are Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![result](Labeled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'ref1'></a>\n",
    "[1] KaggleSecond Annual Data Science Bowl , [Link](https://www.kaggle.com/c/second-annual-data-science-bowl/details/deep-learning-tutorial) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='ref2'></a>\n",
    "[2] Jonathan long , Evan Schelhamer, Trevol Darrel, \"Fully Convolutional Networks for Semantic Segmentation\", UC Berkley"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
